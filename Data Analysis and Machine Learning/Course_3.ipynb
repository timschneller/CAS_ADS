{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/neworldemancer/DSF5/blob/master/Course_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to machine learning & Data Analysis\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Data Science Lab, University Of Bern, 2023\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MohpJtH9effm"
      },
      "outputs": [],
      "source": [
        "!pip install scipy=='1.7.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJn0ilgOS8F"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from matplotlib import  pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread\n",
        "from time import time as timer\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from scipy.stats import entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb9uMlBNrEJg"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfprgIx7rI2J"
      },
      "outputs": [],
      "source": [
        "from utils.routines import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHfT21bLjv27"
      },
      "source": [
        "# 1. Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUus-6PW95xA"
      },
      "source": [
        "## 1. K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "T0sSzs5h95xC"
      },
      "source": [
        "### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "XJj9Ry6M95xF"
      },
      "source": [
        "**Objective:** \n",
        "\n",
        "Clustering techniques divide the set of data into group of atoms having common features. Each data point $p$ gets assigned a label $l_p \\in \\{1,..,K\\}$. In this presentation the data points are supposed to have $D$ features, i.e. each data point belongs to $\\mathbf{R}^D$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "5XsYvuoUeffq"
      },
      "source": [
        "**Domains:** \n",
        "\n",
        "- `Cluster centers`. Each cluster center belongs to $\\mathbf{R}^D$. It belongs therefore to the same input space of the datacloud. All combined the cluster centers belong to $\\mathbf{R}^{D \\times N_{clust}}$ and can be saved in a matrix of dimension $D \\times N_{clust}$\n",
        "\n",
        "\n",
        "- `Label assignement`. Each point must be assigned to a label. Given an ordering of the points, a cluster \n",
        "assignement can be represented therefore by a sequence $\\{l_1,...,l_{N_{points}}\\}$. Formally a label assigenemt belongs to $\\{0,...,N_{clust}-1\\}^{N_{points}}.$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wOaFNbi1effq"
      },
      "source": [
        "**Methods:** \n",
        "- We call $P_k$ the subset of the data set which gets assigned to class $k \\in \\{0,...,N_{clust}-1\\}$ . \n",
        "K-means aims at minimizing the objective function among all possible label assignments and all possible cluster centers $\\{\\mathbf{c}_k\\}_{k=1,..,N_k}$:\n",
        "\n",
        "$$L(\\text{class_assigment}, \\{\\mathbf{c}_k\\}) = \\sum_k L_k (\\text{class_assigment}, \\{\\mathbf{c}_k\\})$$\n",
        "$$L_k= \\sum_{p \\in P_k} d(\\mathbf{x}_p,\\mathbf{c}_{k})^2$$\n",
        "\n",
        "where d is the metric function suited of the problem of interest. Here we use the Euclidean distance as a metric :\n",
        "\n",
        "$$d(\\mathbf{x}_p,\\mathbf{c}_{k})=|\\mathbf{x}_p-\\mathbf{c}_{k}| \\quad \\quad $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "HU7bEAPBeffr"
      },
      "source": [
        "We proceed using a simple technique called \"coordinate descent\", in which we fix one coordinate and optimize the other. Than we iterate. To do this we have to answer two questions:\n",
        "\n",
        "QUESTION 1: Given some cluster centers, what is the optimal class assignement ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "kYnM_75qeffr"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Assignments_1.png\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "xgfCOEB3effr"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Assignments_2.png\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "39gEk50Deffs"
      },
      "source": [
        "ANSWER 1: Each point has to be assigned to the closest cluster center.\n",
        "\n",
        "QUESTION 2: Given a fixed label assignment, what are the optimal cluster centers ?\n",
        "\n",
        "ANSWER 2: If we fix a class_assigment, the optimal $\\mathbf{c}_{k}$ are the centroids given by the expression:\n",
        "$$\\mathbf{c}_k=\\frac{1}{N_k}\\sum_{p \\in P_k} \\mathbf{x_p}$$\n",
        "\n",
        "where $N_k$ is the number of points belonging to cluster $k$ ( magnitude of subset $P_k$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "m00xc6Cneffs"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/iterations.png\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2vciYXuTefft"
      },
      "source": [
        "Such an algorithm finds local minima and may need to be started several times with different initializations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "CQTkkdLnefft"
      },
      "source": [
        "**Terminology and output of a K-means computation:**\n",
        "- *Within-cluster variation* : $L_k$ is called within cluster variation. \n",
        "\n",
        "- *Silhouette score*: K-means clustering fixes the number of clusters a priori. Some technique must be chosen to score the different optimal clusterings for different $k$. One technique chooses the best *Silouhette score*. Intuitively, this evaluates the typical distance of points within a same clusters and compares it against the typical distance of points belonging to neighboring but different clusters ( https://en.wikipedia.org/wiki/Silhouette_(clustering) )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "41kBe_Qfefft"
      },
      "source": [
        "### EXERCISE 1: Implement K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "SFU5M14rLvWE"
      },
      "outputs": [],
      "source": [
        "# The following functions, when completed, provide a naive implementation of K-Means.\n",
        "# Insert your code between the lines # BEGIN OF YOUR CODE // # END OF YOUR CODE\n",
        "# Execute next shell to test your implementation\n",
        "\n",
        "def get_updated_centers( points, current_class_assignment, nclust ):\n",
        "    '''\n",
        "    Input:   \n",
        "    - points : the datacloud, of shape (npoints,ndims)\n",
        "    - current_class_assignmet : an array of shape (npoints) with values in [0,...,nclust-1]\n",
        "    - nclust:  the number of clusters considered\n",
        "    Returns: \n",
        "    - centers : an array of shape (nclust, ndims) containing the updated cluster centers\n",
        "    '''\n",
        "    npoints, ndims = points.shape\n",
        "    # BEGIN OF YOUR CODE\n",
        "    # Initialize the updated cluster centers with zeros\n",
        "    # ...\n",
        "    # END OF YOUR CODE\n",
        "    maps=[]\n",
        "    for clust in range(nclust):\n",
        "        maps.append(current_class_assignment==clust)\n",
        "    # BEGIN OF YOUR CODE\n",
        "    # With \"points[maps[clust]]\" you can access all data points that are currently\n",
        "    # assigned to cluster clust. Use it to initialize centers[clust]\n",
        "    # ...\n",
        "    # END OF YOUR CODE\n",
        "    \n",
        "    return centers \n",
        "\n",
        "\n",
        "def get_upated_class_assignment(points, current_centers, nclust ):\n",
        "    '''\n",
        "    Input:   \n",
        "    - points : the datacloud, of shape (npoints,ndims)\n",
        "    - current_centers : an array of shape (nclust,ndims) containing the cluster centers at present iteration\n",
        "    - nclust:  the number of clusters considered\n",
        "    Returns: \n",
        "    - centers :  an array of shape (npoints) with values in [0,...,nclust-1] with the updated class assignements\n",
        "    '''\n",
        "    npoints, ndims = points.shape\n",
        "    # BEGIN OF YOUR CODE \n",
        "    # Initialize the class_assignment vector as an array of zeros\n",
        "    # ...\n",
        "    # END OF YOUR CODE\n",
        "    for ip in range(npoints):\n",
        "        # BEGIN OF YOUR CODE \n",
        "        # Create a list of all distances from the current point ( with position points[ip,:] and the cluster centers )\n",
        "        # ...\n",
        "        # END OF YOUR CODE\n",
        "        class_assignment[ip]=np.argmin(np.array(distances))   \n",
        "    return class_assignment\n",
        "\n",
        "\n",
        "def KNN_iterator( points, get_updated_centers, get_upated_class_assignment, nclust, n_iter, seed=42 ):\n",
        "    '''\n",
        "    Input:   \n",
        "    - points : the datacloud, of shape (npoints,ndims)\n",
        "    - get_updated_centers, get_upated_class_assignment : The functions you developed in the previous part\n",
        "    - n_clust : the number of clusters considered\n",
        "    - n_iter : the number of iterations\n",
        "    Returns: \n",
        "    - traj :  a list on length n_iter . traj[i] is an array of shape (nclust,ndims) giving the cluster center \n",
        "    positions\n",
        "    '''\n",
        "    npoints, ndims = points.shape\n",
        "    np.random.seed(seed)\n",
        "    centers=np.random.uniform(low=-20.0, high=20.0, size=(nclust,ndims))\n",
        "    traj=[]\n",
        "    traj.append(centers)\n",
        "    # BEGIN OF YOUR CODE . \n",
        "    # Perform the loop over iterations n_iter, calling each time first get_upated_class_assignment and\n",
        "    # than get_updated_centers. Append the centers in the list traj\n",
        "    # ...\n",
        "    # END OF YOUR CODE\n",
        "    return traj\n",
        "\n",
        "\n",
        "def plot_traj(traj):\n",
        "    n_iter=len(traj)\n",
        "    nclust, ndims=traj[0].shape\n",
        "    if ndims != 2 :\n",
        "        print('Today we just visualize dim =2')\n",
        "        return\n",
        "    else :\n",
        "        colors=['red','green','yellow']\n",
        "        for ic in range(nclust):\n",
        "            for it in np.arange(n_iter-1):\n",
        "                plt.plot([traj[it][ic,0],traj[it+1][ic,0]],\n",
        "                         [traj[it][ic,1],traj[it+1][ic,1]],'-o', color=colors[ic])  \n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "mThZ2Jm8effu"
      },
      "outputs": [],
      "source": [
        "# Execute these lines to test your code and see the evolution of the cluster_centers\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "points=km_load_th1()\n",
        "traj=KNN_iterator(points, get_updated_centers, get_upated_class_assignment, nclust=3,n_iter=100, seed=66 )\n",
        "plt.plot(points[:,0],points[:,1],'o')\n",
        "plot_traj(traj)\n",
        "plt.plot(traj[-1][:,0],traj[-1][:,1],'o',markersize=5, color='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "gOmoafxD95xT"
      },
      "source": [
        "### Sklearn: implementation and usage of K-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bxacfstD95xW"
      },
      "source": [
        "We start with a 2D example that can be visualized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "osqtrCM295xn"
      },
      "source": [
        "First we load the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "C6X5O32r95xp"
      },
      "outputs": [],
      "source": [
        "points=km_load_th1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_BTlSM4W95x0"
      },
      "source": [
        "Explore the data-set checking the dataset dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "LsW9d6x_95x3"
      },
      "outputs": [],
      "source": [
        "print(points.shape)\n",
        "print('We have ', points.shape[0], 'points with two features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "usXcr6XP95yD"
      },
      "outputs": [],
      "source": [
        "plt.plot(points[:,0],points[:,1],'o')\n",
        "plt.xlabel('feature-1')\n",
        "plt.ylabel('feature-2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_kLezqgz95yQ"
      },
      "source": [
        "It looks visually that the data set has three clusters. We will cluster them using K-means. As usual, we create a KMeans object. Note that we do not need to initialize it with a data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "5cjLh0ZX95yS"
      },
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=3, random_state=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "fHf2oH7p95yf"
      },
      "source": [
        "A call to the fit method computes the cluster centers which can be plotted alongside the data-set. They are accessible from the cluster_centers_ attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ET3C5t-r95yh"
      },
      "outputs": [],
      "source": [
        "clusterer.fit(points)\n",
        "plt.plot(points[:,0],points[:,1],'o')\n",
        "plt.plot(clusterer.cluster_centers_[:,0],clusterer.cluster_centers_[:,1],'o',markersize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "z4ou8qEVLvWy"
      },
      "outputs": [],
      "source": [
        "clusterer.cluster_centers_[:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "J7H4hw4l95yu"
      },
      "source": [
        "The predict method assigns a new point to the nearest cluster. We can use predict with the training dataset and color the data-set according to the cluster label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "BqPcVkcR95yw"
      },
      "outputs": [],
      "source": [
        "cluster_labels=clusterer.predict(points)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wBtdsqxK95y6"
      },
      "source": [
        "Finally, we can try to vary the number of clusters and score them with the Silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "_dg8v1ST95y8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sil=[]\n",
        "\n",
        "for iclust in range(2,6):\n",
        "    clusterer = KMeans(n_clusters=iclust, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(points)\n",
        "    score=silhouette_score(points,cluster_labels)\n",
        "    sil.append(score)\n",
        "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "    plt.show()\n",
        "    \n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.plot(np.arange(len(sil))+2, sil,'-o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8RVDOdZB95zI"
      },
      "source": [
        "The same techniques can be used on high dimensional data-sets. We use here the famous MNIST dataset for integer digits, that we are downloading from tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "RSzh1J9q95zJ"
      },
      "outputs": [],
      "source": [
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "X=train_images[:5000,:].reshape(5000,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "FTguyfEH95zV"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "image=X[1232,:].reshape(28,28)\n",
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "08EWsPIR95zh"
      },
      "source": [
        "We can cluster the images exactly as we did for the 2d dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "-TU70SxP95zm"
      },
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=10, random_state=10)\n",
        "cluster_labels = clusterer.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Q2QNHiFt95zx"
      },
      "source": [
        "We can plot the cluster centers (which are 2D figures!) to see if the clustering is learning correct patterns! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ba_LXoum95z0"
      },
      "outputs": [],
      "source": [
        "for iclust in range(10):\n",
        "    plt.imshow(clusterer.cluster_centers_[iclust].reshape(28,28))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1QTiCQE395z9"
      },
      "source": [
        "You can see that the model looks to assign one class to the same good. Nevertheless, using the cluster centers and with a further trick, in exercise 2 you will build a digit recognition model !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "jInWo3Uk95z_"
      },
      "source": [
        "### EXERCISE 2: Discover the number of Gaussians"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "FZSVE8wfeffz"
      },
      "outputs": [],
      "source": [
        "### In this exercise you are given the dataset points, consisting of high-dimensional data. It was built taking random \n",
        "# samples from a number k of multimensional gaussians. The data is therefore made of k clusters but, being \n",
        "# very high dimensional, you cannot visualize it. Your task it to use K-means combined with the Silouhette \n",
        "# score to find the number of k.\n",
        "\n",
        "# 1. Load the data using the function points=load_ex1_data_clust() , check the dimensionality of the data.\n",
        "points= ...\n",
        "npoints, ndims = ...\n",
        "print(npoints,ndims)\n",
        "\n",
        "# 2. Fix the number of clusters k and define a KMeans clusterer object. Perform the fitting and compute the Silhouette score. \n",
        "# Save the results on a list. \n",
        "res= ...\n",
        "for n_clusters in ... :\n",
        "    clusterer = ...\n",
        "    cluster_labels = ...\n",
        "    score= ...\n",
        "    res.append(score)\n",
        "    \n",
        "# 3. Plot the Silhouette scores as a function ok k? What is the number of clusters ?\n",
        "plt.plot(...,...,'-o')\n",
        "\n",
        "# 4. Optional. Check the result that you found via umap. Remember the syntax umap_model=umap.UMAP(random_state=xxx) to \n",
        "# istantiate the umap model and than use fit_transform to compute the coordinate with the reduced dimensionality. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "v9TSuObo950Z"
      },
      "source": [
        "### EXERCISE 3: Predict the garment using K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "PSyJzomm950e"
      },
      "source": [
        "###### DESCRIPTION ###############\n",
        "\n",
        "In this exercise you are asked to use the clustering performed by K-means to predict the good in the F-mnist dataset. We show here how clustering can be used as a preprocessing tool for a supervised task !\n",
        "\n",
        "We will follow the pipeline to fit the model :\n",
        "\n",
        "1- We perform K-means clustering using just the input data and fixing for the start the number of clusters to 10 ;\n",
        "\n",
        "2- To each cluster, we will attach a label, finding the most represented good inside that cluster. Let's call that label\n",
        " assignment[c] for cluster c ;  \n",
        " \n",
        "When using the model for prediction of a new image we will :\n",
        "\n",
        "1- Find the cluster center nearest to the new image ;\n",
        "\n",
        "2- Assign the new image to the good most represented in that cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4JuuvrRIeff0"
      },
      "outputs": [],
      "source": [
        "# Follow the following STEPS to solve the exercise\n",
        "\n",
        "# STEP 1. Load the dataset and reshape it accordingly.\n",
        "fmnist = ...\n",
        "(train_images, train_labels), (test_images, test_labels) = ...\n",
        "X_train=train_images[:5000,:].reshape(...,...)\n",
        "y_train=train_labels[:5000]\n",
        "X_test=test_images[:1000,:].reshape(...,...)\n",
        "y_test=test_labels[:1000]\n",
        "\n",
        "\n",
        "# STEP 2. \n",
        "# Define the cluster KMeans object and fit the model on the training set. \n",
        "clusterer = ...\n",
        "clusterer.fit(...)\n",
        "\n",
        "# STEP 3. \n",
        "# Compute the cluster labels. \n",
        "cluster_labels=...\n",
        "\n",
        "# STEP 4. \n",
        "# Compute the assignment list. assignment[i] will be the majority class of the i-cluster  \n",
        "# You can use, if you want,  the function most_common with arguments (k,y_train, cluster_labels) \n",
        "# this compute the assignment list. Print the assignment list to explore its values.\n",
        "\n",
        "def most_common(nclusters, supervised_labels, cluster_labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - nclusters : the number of clusters\n",
        "    - supervised_labels : for each garment, the labelling provided by the training data ( e.g. in y_train or y_test)\n",
        "    - cluster_labels : for each garment, the cluster it was assigned by K-Means using the predict method of the Kmeans object\n",
        "    \n",
        "    Returns:\n",
        "    - a list \"assignment\" of lengths nclusters, where assignment[i] is the majority class of the i-cluster \n",
        "    \"\"\"\n",
        "    assignment=[]\n",
        "    for icluster in range(nclusters):\n",
        "        indices=list(supervised_labels[cluster_labels==icluster])\n",
        "        try:\n",
        "            chosen= max(set(indices), key=indices.count)\n",
        "        except ValueError :\n",
        "            print('Em')\n",
        "            chosen=1\n",
        "        assignment.append(chosen)\n",
        "    return assignment\n",
        "assignment=...\n",
        "\n",
        "# STEP 5. \n",
        "# Predict the cluster labels for the test set \n",
        "cluster_labels_test=...\n",
        "\n",
        "# STEP 6.\n",
        "# using the cluster labels predicted in STEP 5 and the previously computed assignment[] list, \n",
        "# predict what are according to your model the predicted goods for the test set, call them new_labels\n",
        "# (The Python notation suggested is called list_comprehension)\n",
        "new_labels = [assignment[...] for ... in ... ]\n",
        "\n",
        "# STEP 7.\n",
        "# Using  a call cm=metrics.confusion_matrix( y_test, new_labels ) you can print the confusion matrix on the test set, \n",
        "# which\n",
        "# provides information on the quality of the fit. Print the percentage of correctly classified examples. \n",
        "# For example, you can divide the sum of the elements on the diagonal of cm (save it in denominator \"den\") \n",
        "# and divide by the sum of all entries of cm (save it in numerator \"num\").  \n",
        "cm= ...\n",
        "print(cm)\n",
        "num=...\n",
        "den=...\n",
        "accuracy=num/den\n",
        "print(num/den)\n",
        "\n",
        "#  Perform again steps 2 / 7 increasing the number of clusters from 10 to 40 what happens to the performance ? \n",
        "for iclust in ... :\n",
        "    print(f'Test set with {iclust} clusters')\n",
        "    # BEGIN OF YOUR CODE . \n",
        "    # ...\n",
        "    # END OF YOUR CODE\n",
        "    print(assignment)\n",
        "    print(accuracy)\n",
        "    print(cm)\n",
        "    return traj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "b3VoJKsVeff0"
      },
      "source": [
        "## 2. Dendograms "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PfeYW8eKeff0"
      },
      "source": [
        "**Objective:** \n",
        "\n",
        "In hierarchical clustering we do not have only an optimal set of clusters, but for each different \"length scale\" we have a different set of clusters.\n",
        "\n",
        "**Algorithm:** \n",
        "\n",
        "- We start with a length scale $l=0$ at the beginning and consider all sample elements as different clusters.\n",
        "\n",
        "- We increase than $l$ to values larger than zero. Let's call the minimum distance between pair of points $l_0$. As soon as we reach $l=l_0$, these elements are merged into a new cluster (greedy strategy). \n",
        "\n",
        "To proceed further we need to define a distance between subsets $S_1,S_2$ of points. In the \"single linkage\" flavour we define:\n",
        "\n",
        "$$d(S_1,S_2)=min_{a\\in S_1, b\\in S_2} d(a,b)$$\n",
        "\n",
        "- This way we can proceed increasing $l>l_0$. As soon as we find two clusters with distance smaller than $l_1>l_0$, we merge them. \n",
        "\n",
        "- We keep on increasing $l$ as far as one one cluster remains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yDf_mhqdeff0"
      },
      "source": [
        "The result of this clustering procedure can be summarized in a `dendrogram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "xhNhBihUeff0"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from scipy.cluster import hierarchy\n",
        "from ipywidgets import interact\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "points=km_load_th1()\n",
        "Z = hierarchy.linkage(points, 'single')\n",
        "n_clusters=int(np.max(Z[:,[0,1]].flatten()))\n",
        "print(n_clusters)\n",
        "\n",
        "@interact\n",
        "def plot(t=(0,7,0.1)):  \n",
        "    fig, axes=plt.subplots(1,2,figsize=(15,10), gridspec_kw={'width_ratios': [1, 2]})\n",
        "    fl = fcluster(Z,t=t,criterion='distance')\n",
        "    maps={}\n",
        "    for clust in range(n_clusters):\n",
        "        maps[clust]=(fl==clust)\n",
        "    c=0\n",
        "    for clust in range(n_clusters):\n",
        "        if np.sum(maps[clust])>0:\n",
        "            c+=1\n",
        "    plt.figure(figsize=(19,5))\n",
        "    d = hierarchy.dendrogram(Z,ax=axes[1], color_threshold=t)\n",
        "    axes[1].axhline(t,linestyle='--',color='red')\n",
        "    axes[1].set_ylabel('Cluster distance')\n",
        "    axes[1].axes.get_xaxis().set_visible(False)\n",
        "    axes[1].set_xlabel('Points')\n",
        "    axes[1].set_title(f'Number clusters: {c}')\n",
        "    axes[0].scatter(points[d['leaves'],0],points[d['leaves'],1], color=d['leaves_color_list'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4lTUxeSmeff1"
      },
      "source": [
        "**Properties:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "O98E0rxHeff1"
      },
      "source": [
        "If we fix a certain cutoff length $l_c$, the clusters identified $C_1,...,C_n$ at that length are such that : \n",
        "\n",
        "1- The constintute a disjoint partition of the whole dataset, i.e. they are mutually non intersecting aand each point belongs to a cluster \n",
        "\n",
        "2- The distance between two clusters $d(C_1,C_j)$ is larger than $l_c$ for each $i,j$. \n",
        "\n",
        "( Check of point 2: If it was smaller, take $A,B$ such that $d(A,B)=d(C_1,C_j)=l'<l_c$ and $A \\in C_1$, $B \\in C_2$. But than, when merging clusters at length scale $l'$ the clusters at which $A,B$ belonged to, would have been merged. By construction, after two points are merged into the same cluster at a length scale, they belong to the same cluster at all larger length scales. This is a contradiction. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd9WGjVQ951B"
      },
      "source": [
        "## 3. Gaussian mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "baYX-IKOB461"
      },
      "source": [
        "### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "jkueVtkf951D"
      },
      "source": [
        "K-Means is a modelling procedure which is biased towards clusters of circular shape and therefore does not always work perfectly, as the following examples show:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "S_kqTtgi951F"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th1()\n",
        "clusterer = KMeans(n_clusters=3, random_state=10)\n",
        "cluster_labels=clusterer.fit_predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.title('K-Means')\n",
        "plt.xlim(-6,6)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "bRsc4ZOV951O"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th2()\n",
        "clusterer = KMeans(n_clusters=2, random_state=10)\n",
        "cluster_labels=clusterer.fit_predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title('K-Means')\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4SvVyxXu952E"
      },
      "source": [
        "A Gaussian mixture model is able to fit these kinds of clusters. In a Gaussian mixture model, each data-set is supposed to be a random point from the distribution:\n",
        "$$f(\\mathbf{x})=\\sum_c \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$$\n",
        ", which is called a Gaussian mixture (N stands for Normal distribution). The parameters $\\{\\pi_c,\\mathbf{\\mu_c},\\mathbf{\\Sigma_c}\\}$ are fitted from the data using a minimization procedure (maximum likelihood via the EM algorithm) and $N_c$ is the chosen number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "oOalY-1veff2"
      },
      "source": [
        "This density has the following interpretation. Suppose each data point $\\hat{X}$ is a random variable resulting from the following 2-step procedure:\n",
        "\n",
        "1- First a random variable $\\hat {C}$ is extracted, with values in $\\{1,...,N_c\\}$ and probabilities $\\{\\pi_1,...\\pi_{N_c}\\}$. This random variable fixes a cluster.\n",
        "\n",
        "2- When we know $\\hat {C}=c$ we extract $\\hat{X}$ according to the density $N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$.\n",
        "\n",
        "This procedure is summarized by the formulas:\n",
        "\n",
        "$$p(\\hat{C}=c)=\\pi_c$$\n",
        "\n",
        "$$p(\\hat{X}=\\mathbf{x}|\\hat{C}=c)=N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c})(\\mathbf{x})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wqOLzPso952G"
      },
      "source": [
        "**Output of a GM computation:** \n",
        "- *Cluster probabilities:* A gaussian mixtures model is an example of soft clustering, where each data point $p$ does not get assigned a unique cluster, but a distribution over clusters $f_p(c), c=1,...,N_c$. \n",
        "\n",
        "Given the fitted parameters,  $f_p(c)$ is computed as: $$f_p(c)=\\frac{ \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x_p})}{\\sum_{c'} \\pi_c N(\\mathbf{\\mu_{c'}},\\mathbf{\\Sigma_{c'}} )(\\mathbf{x_p})}, c=1...N_c$$ \n",
        "\n",
        ", where $\\mathbf{x_p}$ are the coordinates of point p. \n",
        "\n",
        "This formula follows from our probabilistic interpretation and Bayes theorem:\n",
        "\n",
        "$$f_p(c)=p(\\hat{C}=c|\\hat{X}=\\mathbf{x_p}) \\sim p(\\hat{X}=\\mathbf{x_p}|\\hat{C}=c) p(\\hat{C}=c)=N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x_p})\\pi_c $$\n",
        "\n",
        "- *AIC/BIC:* after each clustering two numbers are returned. These can be used to select the optimal number of Gaussians to be used, similar to the Silhouette score. ( AIC and BIC consider both the likelihood of the data given the parameters and the complexity of the model related to the number of Gaussians used ). The lowest AIC or BIC value is an indication of a good fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "fdddJhgl952H"
      },
      "source": [
        "### Sklearn: implementation and usage of Gaussian mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PCyazHHp952W"
      },
      "source": [
        "First of all, we see how the Gaussian model behaves on our original example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "MTu1jVUr952Y",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "points=km_load_th1()\n",
        "\n",
        "aic=[]\n",
        "bic=[]\n",
        "sil=[]\n",
        "\n",
        "for i_comp in range(2,6):\n",
        "    plt.figure()\n",
        "    plt.title(str(i_comp))\n",
        "    clf = GaussianMixture(n_components=i_comp, covariance_type='full')\n",
        "    clf.fit(points)\n",
        "    cluster_labels=clf.predict(points)\n",
        "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "    print(i_comp,clf.aic(points),clf.bic(points))\n",
        "    score=silhouette_score(points,cluster_labels)\n",
        "    aic.append(clf.aic(points))\n",
        "    bic.append(clf.bic(points))\n",
        "    sil.append(score)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "iVTi69ZS952o"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(2,6),aic,'-o')\n",
        "plt.title('aic')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(2,6),bic,'-o')\n",
        "plt.title('bic')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(2,6),sil,'-o')\n",
        "plt.title('silhouette')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "W_RrKQ0W952z"
      },
      "source": [
        "So in this case we get a comparable result, and also the probabilistic tools agree with the Silhouette score ! Let's see how the Gaussian mixtures behave in the examples where K-means was failing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "cKmBGlDr9521"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th1()\n",
        "clf = GaussianMixture(n_components=3, covariance_type='full')\n",
        "clf.fit(points)\n",
        "cluster_labels=clf.predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.title('K-Means')\n",
        "plt.xlim(-6,6)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4ubCBvfj952-"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th2()\n",
        "clf = GaussianMixture(n_components=2, covariance_type='full')\n",
        "clf.fit(points)\n",
        "cluster_labels=clf.predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIykj5dp953I"
      },
      "source": [
        "### EXERCISE 4: Find the prediction uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-y4HI-veff4"
      },
      "outputs": [],
      "source": [
        "#In this exercise you need to load the dataset used to present K-means ( def km_load_th1() ) or the one used to discuss \n",
        "# the Gaussian mixtures model ( def gm_load_th1() ). \n",
        "#As discussed, applying a fitting based on gaussian mixtures you can not only predict the cluster label for each point, \n",
        "#but also a probability distribution over the clusters. \n",
        "\n",
        "#From this probability distribution, you can compute for each point the entropy of the corresponging \n",
        "#distribution (using for example scipy.stats.entropy) as an estimation of the undertainty of the prediction. \n",
        "#Your task is to plot the data-cloud with a color proportional to the uncertainty of the cluster assignement.\n",
        "\n",
        "# In detail you shoud:\n",
        "# 1. Instantiate a GaussianMixture object with the number of clusters that you expect\n",
        "# 2. fit the object on the dataset with the fit method \n",
        "\n",
        "points=gm_load_th1()\n",
        "\n",
        "plt.figure()\n",
        "clf = GaussianMixture(..., covariance_type='full')\n",
        "clf...(...)\n",
        "\n",
        "# 3. compute the cluster probabilities using the method predict_proba. This will return a matrix of \n",
        "# dimension npoints x nclusters (check that this is the case!)\n",
        "# 4. use the entropy function ( from scipy.stats you need to import the entropy \n",
        "# function ) to evaluate for each point the uncertainty of the \n",
        "#prediction. Check here if in doubt: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
        "\n",
        "cluster_labels_prob=...\n",
        "\n",
        "from ... import ...\n",
        "entropies=[]\n",
        "for point in range(len(...)):\n",
        "    entropies.append(...)\n",
        "\n",
        "# 5. Plot the points colored accordingly to their uncertanty. \n",
        "\n",
        "cm = plt.cm.get_cmap('RdYlBu')\n",
        "sc = plt.scatter(points[:,0], points[:,1], c=entropies, cmap=cm)\n",
        "plt.colorbar(sc)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "9O3ArMG6eff4"
      },
      "source": [
        "# Final comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3EbOD7_Eeff5"
      },
      "source": [
        "We covered here the most basic clustering techniques, showcasing different behaviors. For real like projects, there are also other algorithms that could be taken into consideration, e.g.:\n",
        "\n",
        "- DBSCAN : https://en.wikipedia.org/wiki/DBSCAN\n",
        "- Spectral Clustering : https://towardsdatascience.com/spectral-clustering-aba2640c0d5b\n",
        "\n",
        "Both these methods can fit also clusters with weirder shapes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dE4fVdZG953V",
        "uHfT21bLjv27",
        "pUus-6PW95xA",
        "T0sSzs5h95xC",
        "gOmoafxD95xT",
        "jInWo3Uk95z_",
        "v9TSuObo950Z",
        "qd9WGjVQ951B",
        "baYX-IKOB461",
        "fdddJhgl952H",
        "IIykj5dp953I",
        "oAAjJuenj1u0",
        "5AXSFimKkt91",
        "C-mH0li3kzNi",
        "8QZtmo6Vk2rp",
        "u62hCoFbklaW",
        "yrwaonuB4F8m",
        "aOk-BLOFeV61"
      ],
      "name": "Course_3",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
